# -*- coding: utf-8 -*-
"""Week3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1AYBMux3W7c9rVd5a61BndQDsIxGh4Env
"""

import torch
import torch.nn as nn
import torch.optim as optim
import matplotlib.pyplot as plt

# 生成訓練資料
x = torch.linspace(-1, 1, 2000).reshape(-1, 1)
y = 1 / (1 + 25 * x**2)
dy = -50 * x / (1 + 25 * x**2)**2

# 訓練/驗證分割
split = int(0.8 * len(x))
x_train, x_val = x[:split], x[split:]
y_train, y_val = y[:split], y[split:]
dy_train, dy_val = dy[:split], dy[split:]

# 定義神經網路
class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        self.net = nn.Sequential(
            nn.Linear(1, 20),
            nn.Tanh(),
            nn.Linear(20, 20),
            nn.Tanh(),
            nn.Linear(20, 2)   # 輸出: [f(x), f'(x)]
        )
    def forward(self, x):
        return self.net(x)

model = Net()
optimizer = optim.Adam(model.parameters(), lr=1e-3)

# 自定義 loss function
def custom_loss(pred, y_true, dy_true, alpha=1.0, beta=1.0):
    f_pred, df_pred = pred[:,0], pred[:,1]
    loss_func = torch.mean((f_pred - y_true.squeeze())**2)
    loss_deriv = torch.mean((df_pred - dy_true.squeeze())**2)
    return alpha * loss_func + beta * loss_deriv, loss_func, loss_deriv

# 訓練
epochs = 600
train_losses, val_losses = [], []
train_func_losses, train_deriv_losses = [], []

for epoch in range(epochs):
    model.train()
    optimizer.zero_grad()
    pred = model(x_train)
    loss, lf, ld = custom_loss(pred, y_train, dy_train)
    loss.backward()
    optimizer.step()

    model.eval()
    with torch.no_grad():
        val_pred = model(x_val)
        val_loss, _, _ = custom_loss(val_pred, y_val, dy_val)

    train_losses.append(loss.item())
    val_losses.append(val_loss.item())
    train_func_losses.append(lf.item())
    train_deriv_losses.append(ld.item())

# 評估
model.eval()
with torch.no_grad():
    preds = model(x)
    y_pred, dy_pred = preds[:,0], preds[:,1]
    mse = torch.mean((y_pred - y)**2).item()
    max_err = torch.max(torch.abs(y_pred - y)).item()
    mse_dy = torch.mean((dy_pred - dy)**2).item()
    max_err_dy = torch.max(torch.abs(dy_pred - dy)).item()


# 繪製圖形
plt.figure(figsize=(14,5))

plt.subplot(1,3,1)
plt.plot(x, y, label="True f(x)")
plt.plot(x, y_pred, "--", label="NN f(x)")
plt.plot(x, dy, label="True f'(x)")
plt.plot(x, dy_pred, "--", label="NN f'(x)")
plt.legend(); plt.title("Runge Function and Derivative")

plt.subplot(1,3,2)
plt.plot(train_losses, label="Train Loss")
plt.plot(val_losses, label="Val Loss")
plt.yscale("log")
plt.legend(); plt.title("Total Loss")

plt.subplot(1,3,3)
plt.plot(train_func_losses, label="Function Loss")
plt.plot(train_deriv_losses, label="Derivative Loss")
plt.yscale("log")
plt.legend(); plt.title("Components of Loss")

plt.tight_layout()
plt.show()

print(f"MSE of f(x): {mse:.5e}, Max Error of f(x): {max_err:.5e}")
print(f"MSE of f'(x): {mse_dy:.5e}, Max Error of f'(x): {max_err_dy:.5e}")

